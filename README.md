# RacingDNA

Formula 1 driving style analysis using neural networks (VAE/AutoEncoder) to cluster corner telemetry data.

## The Project

The core of the project is to analyze Formula 1 telemetry to identify and classify drivers' cornering styles. The workflow is structured as follows:

1.  **Corner Extraction** (`src/analysis/CurveDetector.py`):
    Raw telemetry is analyzed to extract corner segments. This is done using:
    -   **Spatial Bounds**: Definition of an area of interest around the corner apex based on the circuit map (`corners.json`).
    -   **Windowing and Smoothing**: Application of a moving average on lateral acceleration (`acc_y`) to reduce noise.
    -   **Hysteresis Thresholding**: Corner entry is detected when G-force exceeds a threshold (`ACC_ENTER_THR`, default 3.0 G), while exit occurs when it drops below a lower threshold (`ACC_EXIT_THR`, default 2.5 G), ensuring detection stability.
    -   **Tire Analysis**: To extract compound and tire life information, the `laptimes.json` file must be present in the same folder as the telemetry.

> [!IMPORTANT]
> **Key Requirement for Tire Analysis**:
> For the script to correctly extract tire **compound** and **life**, it is **mandatory** that the `laptimes.json` file be present in the **same folder** as the telemetry file (e.g. `1_tel.json`). Without this file, tire information **will not be available**.

2.  **Dataset Construction and Normalization** (`src/analysis/dataset_normalization.py`):
    Extracted corners are collected into a dataset. A **Z-score normalization** (subtracting the mean and dividing by the standard deviation) calculated on the entire dataset is applied to make the data homogeneous and suitable for neural network training.

3.  **Modelling with VAE** (`src/models/VAE.py`, `src/models/AutoEncoder.py`):
    A **VAE (Variational AutoEncoder)** (or standard AutoEncoder) neural network is trained on the normalized dataset. The VAE learns to compress complex corner data (speed, braking, acceleration, trajectory) into a compact representation called **latent space**.

4.  **Clustering and Driving Style** (`src/scripts/train.py`):
    The latent space generated by the Encoder is divided into **4 clusters** using the **K-Means** algorithm. These clusters represent different driving styles (e.g. aggressive, saving, consistent, etc.), allowing automatic labeling of driver behavior in each corner.

### Input and Output by Script

Below are the details of specific inputs and outputs for each main project script.

#### 1. `src/scripts/train.py` (Training and Clustering)
*   **Input**:
    *   **Normalized Dataset**: `.npz` file containing processed corners (automatically downloadable from Hugging Face).
    *   **Configuration**: Parameters defined in `TrainConfig` (e.g. `latent_dim`, `num_clusters`, `epochs`).
    *   **Pre-trained Weights** (optional): `.pth` file if choosing not to retrain the model.
    *   **Centroids** (optional): `.npy` file if loading an existing clustering.
*   **Output**:
    *   **Trained Model**: Weights file `.pth` (if `train_model = True`).
    *   **K-Means Centroids**: `.npy` file saved after clustering.
    *   **Visualizations**: 2D/3D plots of latent space and clusters.
    *   **Cluster Statistics**: Console print of cluster distribution and characteristics.

#### 2. `src/scripts/evaluate.py` (Telemetry Evaluation)
*   **Input**:
    *   **Raw Telemetry**: Session `.json` file (e.g. `1_tel.json`).
    *   **Laptimes**: `laptimes.json` file in the same folder as telemetry (needed for compound and tire life info).
    *   **Corner Map**: `corners.json` file relative to the circuit.
    *   **Model**: VAE/AutoEncoder weights (`.pth`).
    *   **Normalized Dataset**: Required to load statistics (mean/std) used for normalization.
    *   **Centroids**: `.npy` file to assign clusters.
*   **Output**:
    *   **Classification**: Console print of corner-by-corner sequence with assigned cluster and predominant driving style.
    *   **Analysis**: Usage percentages of various driving styles in the session.

#### 3. `src/analysis/curve_visualizer.py` (Corner Visualization)
*   **Input**:
    *   **Raw Telemetry**: Session `.json` file.
    *   **Laptimes**: `laptimes.json` file in the same folder as telemetry (needed for compound and tire life info).
    *   **Corner Map**: Circuit `corners.json` file.
*   **Output**:
    *   **Signal Plots**: Plot of telemetry channels (Lateral Acceleration, Brake, Throttle) for each detected corner.
    *   **Trajectories**: 2D visualization of the trajectory taken in each corner relative to the track.
    *   **Debug**: Visual verification of bound correctness and corner detection.

## Installation

**IDE used for development**: Visual Studio Code
**Python version**: 3.13.3

```bash
# 1. Clone the repository
git clone https://github.com/FlorindoDev/RacingDNA.git
cd RacingDNA

# 2. Create virtual environment (recommended)
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# 3. Install dependencies
pip install -r requirements.txt

# 4. Install the package
pip install -e .
```

## Dataset

The complete dataset used for training this model, including normalization and detailed structure, is available on Hugging Face:
[**FlorindoDev/f1_corner_telemetry_2024_2025**](https://huggingface.co/datasets/FlorindoDev/f1_corner_telemetry_2024_2025)

> [!NOTE]
> **Credits**: Original raw telemetry data was obtained from [**TracingInsights-Archive**](https://github.com/TracingInsights-Archive).

For details on the `.csv` file structure, corner extraction logic, padding, and normalization, see the dedicated documentation:
[**Go to Dataset Documentation**](data/dataset/README.md)

## Usage

The project offers ready-to-use scripts for model training and telemetry evaluation.

> [!TIP]
> **Ready to Run**:
> All three scripts are configured to work **immediately** without any modification. Just run the command, and the code will automatically handle data downloading (from Hugging Face), model loading, and execution using default parameters.

### 1. Training and Clustering

The `src/scripts/train.py` script is the heart of the project and manages several phases:
1.  **Training**: Trains the model (AutoEncoder or VAE) to learn to represent corners.
2.  **Encoding**: Uses the trained model to compress all dataset corners into latent space.
3.  **Clustering**: Applies K-Means algorithm on latent space to identify groups (clusters) of similar driving styles.
4.  **Visualization**: Generates 2D/3D plots of latent space and clusters.

> [!IMPORTANT]
> **Note on Automatic Download**:
> The code already includes flags (`download_from_hf = True`) in configuration files (`train.py`, `evaluate.py`) enabled by default. This means that by starting the scripts, the correct and normalized dataset will be automatically downloaded from Hugging Face, ensuring total compatibility with provided weights.
>
> **Note on Pre-trained Weights**:
> If deciding not to use automatic download, to obtain consistent results using included weights (`src/models/weights/VAE_32z_weights.pth`) and centroids (`src/models/weights/kmeans_centroids.npy`), it is **essential** to manually use the specific dataset available here:
> [**f1_corner_telemetry_2024_2025/tree/main**](https://huggingface.co/datasets/FlorindoDev/f1_corner_telemetry_2024_2025/tree/main).
>
> Using a different dataset or locally re-normalized one will invalidate weights and centroids.
>
> **Cluster Meaning (Default Configuration)**:
> Using included weights and centroids, the 4 identified clusters have the following meaning:
> - **Cluster 0**: Fast Pace.
> - **Cluster 1**: Pushing.
> - **Cluster 2**: Slow Pace.
> - **Cluster 3**: Saving.

**What you can do**:
- **Configure paths**: Modify `dataset_path`, `load_weights_path` etc. in `TrainConfig`.
- **Train from scratch or load**: Set `train_model = True` to retrain, `False` to use saved weights.
- **Choose model**: `use_vae = True` for Variational AutoEncoder, `False` for standard AutoEncoder.
- **Modify clusters**: Change `num_clusters` to search for more or fewer driving styles.
- **Manage Centroids**: Use `save_centroids = True` to save calculated centroids for reuse, or `load_centroids = True` to load existing centroids without re-clustering (useful if you have defined clusters and want to maintain consistency).
- **Enable plots**: Set flags like `show_latent_space_2d` or `show_clusters_3d` to `True` to see results.

```bash
python -m src.scripts.train
```

**Automatic Download from Hugging Face**:

You can enable automatic download of normalized dataset by setting `download_from_hf = True` in the config.

### 2. Single Session Evaluation

The `src/scripts/evaluate.py` script is used to apply the trained model to **new data** (e.g. a specific driver's lap).

**How it works**:
1.  Loads the specified raw telemetry file (`.json`).
2.  Uses `corners.json` file to locate corners on the track.
3.  Extracts and normalizes corners using original dataset statistics.
4.  Projects each corner into latent space and assigns it to the nearest cluster.

**Automatic Download from Hugging Face**:

The script supports automatic data download:
- `download_from_hf = True`: Downloads normalized dataset
- `download_raw_from_hf = True`: Downloads raw telemetry data (folder `2024-main` or `2025-main`)

**What you can do**:
- **Analyze a driver**: Change `telemetry_path` to point to the session JSON file.
- **Compare styles**: Run script on different laps to see if style changes.
- **Verify output**: Script prints assigned cluster ID for each corner.

```bash
python -m src.scripts.evaluate
```

### 3. Corner Visualization

The `src/analysis/curve_visualizer.py` script allows visualizing corners detected from a telemetry file.

**What you can do**:
- Graphically see corners extracted from a session
- Visualize corner trajectories on the track
- Automatically download raw data from HF with `download_from_hf = True`

```bash
python -m src.analysis.curve_visualizer
```

### 4. Web Dashboard & Telemetry Analysis

The project includes a comprehensive web interface for analyzing telemetry, laps, and comparing drivers.

**Option A: Docker (Recommended)**
Run the full stack (Back-end + Web Server) in a container.

1.  **Start**:
    ```bash
    cd site
    docker-compose up --build -d
    ```
2.  **Access**: Open `http://localhost` in your browser.

**Option B: Local Development**
1.  **Start**:
    ```bash
    python site/api_server.py
    ```
2.  **Access**: Open `http://localhost:5050` in your browser.

## Project Structure

```
src/
â”œâ”€â”€ analysis/       # Corner detection, preprocessing and curve_visualizer.py
â”œâ”€â”€ models/         # AutoEncoder/VAE, dataset_loader.py (with HF download)
â””â”€â”€ scripts/        # train.py, evaluate.py
data/
â””â”€â”€ dataset/        # Normalized datasets and documentation
```

---

<center>

# ðŸ‡®ðŸ‡¹ Italian Version ðŸ‡®ðŸ‡¹

</center>

---

# RacingDNA

Analisi dello stile di guida in Formula 1 utilizzando reti neurali (VAE/AutoEncoder) per clusterizzare i dati telemetrici delle curve.

## Il Progetto

Il cuore del progetto consiste nell'analizzare le telemetrie di Formula 1 per identificare e classificare lo stile di guida dei piloti in curva. Il flusso di lavoro Ã¨ strutturato come segue:

1.  **Estrazione Curve** (`src/analysis/CurveDetector.py`):
    Le telemetrie grezze vengono analizzate per estrarre i segmenti relativi alle curve. Questo avviene utilizzando:
    -   **Bound spaziali**: Definizione di un'area di interesse attorno all'apice della curva basata sulla mappa del circuito (`corners.json`).
    -   **Finestre e Smoothing**: Applicazione di una *moving average* sull'accelerazione laterale (`acc_y`) per ridurre il rumore.
    -   **Thresholding con Isteresi**: L'ingresso in curva Ã¨ rilevato quando la forza G supera una soglia (`ACC_ENTER_THR`, default 3.0 G), mentre l'uscita avviene quando scende sotto una soglia inferiore (`ACC_EXIT_THR`, default 2.5 G), garantendo stabilitÃ  nella rilevazione.
    -   **Analisi Gomme**: Per estrarre informazioni su mescola e vita degli pneumatici, Ã¨ necessario che il file `laptimes.json` sia presente nella stessa cartella della telemetria.

> [!IMPORTANT]
> **Requisito Fondamentale per Analisi Gomme**:
> Per far sÃ¬ che lo script estragga correttamente la **mescola** e la **vita** degli pneumatici, Ã¨ **obbligatorio** che il file `laptimes.json` sia presente nella **stessa cartella** del file di telemetria (es. `1_tel.json`). Senza questo file, le informazioni sulle gomme **non saranno disponibili**.

2.  **Costruzione e Normalizzazione Dataset** (`src/analysis/dataset_normalization.py`):
    Le curve estratte vengono raccolte in un dataset. Viene applicata una **Z-score normalization** (sottrazione della media e divisione per la deviazione standard) calcolata sull'intero dataset per rendere i dati omogenei e adatti all'addestramento della rete neurale.

3.  **Modellazione con VAE** (`src/models/VAE.py`, `src/models/AutoEncoder.py`):
    Una rete neurale di tipo **VAE (Variational AutoEncoder)** (o AutoEncoder standard) viene addestrata sul dataset normalizzato. Il VAE impara a comprimere i dati complessi della curva (velocitÃ , frenata, accelerazione, traiettoria) in una rappresentazione compatta chiamata **spazio latente**.

4.  **Clustering e Stile di Guida** (`src/scripts/train.py`):
    Lo spazio latente generato dall'Encoder viene suddiviso in **4 cluster** utilizzando l'algoritmo **K-Means**.  questi cluster rappresentano diversi stile di guida (es. aggressivo , gestione, costante, ecc.), permettendo di etichettare automaticamente il comportamento del pilota in ogni curva.

### Input e Output per Script

Di seguito i dettagli degli input e output specifici per ogni script principale del progetto.

#### 1. `src/scripts/train.py` (Training e Clustering)
*   **Input**:
    *   **Dataset Normalizzato**: File `.npz` contenente le curve processate (scaricabile automaticamente da Hugging Face).
    *   **Configurazione**: Parametri definiti in `TrainConfig` (es. `latent_dim`, `num_clusters`, `epochs`).
    *   **Pesi Pre-addestrati** (opzionale): File `.pth` se si sceglie di non riaddestrare il modello.
    *   **Centroidi** (opzionale): File `.npy` se si vuole caricare una clusterizzazione esistente.
*   **Output**:
    *   **Modello Addestrato**: File dei pesi `.pth` (se `train_model = True`).
    *   **Centroidi K-Means**: File `.npy` salvato dopo il clustering.
    *   **Visualizzazioni**: Grafici 2D/3D dello spazio latente e dei cluster.
    *   **Statistiche Cluster**: Stampa a console della distribuzione e caratteristiche dei cluster.

#### 2. `src/scripts/evaluate.py` (Valutazione Telemetria)
*   **Input**:
    *   **Telemetria Grezza**: File `.json` di una sessione (es. `1_tel.json`).
    *   **Laptimes**: File `laptimes.json` nella stessa cartella della telemetria (necessario per info su mescola e vita gomma).
    *   **Mappa Curve**: File `corners.json` relativo al circuito.
    *   **Modello**: Pesi del VAE/AutoEncoder (`.pth`).
    *   **Dataset Normalizzato**: Necessario per caricare le statistiche (media/std) usate per la normalizzazione.
    *   **Centroidi**: File `.npy` per assegnare i cluster.
*   **Output**:
    *   **Classificazione**: Stampa a console della sequenza curva-per-curva con il cluster assegnato e lo stile di guida predominante.
    *   **Analisi**: Percentuali di utilizzo dei vari stili di guida nella sessione.

#### 3. `src/analysis/curve_visualizer.py` (Visualizzazione Curve)
*   **Input**:
    *   **Telemetria Grezza**: File `.json` della sessione.
    *   **Laptimes**: File `laptimes.json` nella stessa cartella della telemetria (necessario per info su mescola e vita gomma).
    *   **Mappa Curve**: File `corners.json` del circuito.
*   **Output**:
    *   **Grafici Segnali**: Plot dei canali telemetrici (Accelerazione Laterale, Freno, Acceleratore) per ogni curva rilevata.
    *   **Traiettorie**: Visualizzazione 2D della traiettoria percorsa in ogni curva rispetto al tracciato.
    *   **Debug**: Verifica visiva della correttezza dei bound e del rilevamento curve.

## Installazione

**IDE utilizzato per lo sviluppo**: Visual Studio Code
**Versione Python**: 3.13.3

```bash
# 1. Clona il repository
git clone https://github.com/FlorindoDev/RacingDNA.git
cd RacingDNA

# 2. Crea ambiente virtuale (consigliato)
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# 3. Installa dipendenze
pip install -r requirements.txt

# 4. Installa il pacchetto
pip install -e .
```

## Dataset

Il dataset completo utilizzato per il training di questo modello, inclusa la normalizzazione e la struttura dettagliata, Ã¨ disponibile su Hugging Face:
[**FlorindoDev/f1_corner_telemetry_2024_2025**](https://huggingface.co/datasets/FlorindoDev/f1_corner_telemetry_2024_2025)

> [!NOTE]
> **Credits**: I dati raw telemetrici originali sono stati ottenuti da [**TracingInsights-Archive**](https://github.com/TracingInsights-Archive).


Per dettagli sulla struttura del file `.csv`, la logica di estrazione delle curve, il padding e la normalizzazione, consulta la documentazione dedicata:
[**Vai alla documentazione del Dataset**](data/dataset/README.md)

## Utilizzo

Il progetto offre script pronti all'uso per il training del modello e la valutazione della telemetria.

> [!TIP]
> **Pronto all'Uso**:
> Tutti e tre gli script sono configurati per funzionare **immediatamente** senza alcuna modifica. Basta lanciare il comando e il codice gestirÃ  automaticamente il download dei dati (da Hugging Face), il caricamento del modello e l'esecuzione utilizzando i parametri di default.

### 1. Training e Clustering

Lo script `src/scripts/train.py` Ã¨ il cuore del progetto e gestisce diverse fasi:
1.  **Training**: Addestra il modello (AutoEncoder o VAE) per imparare a rappresentare le curve.
2.  **Encoding**: Utilizza il modello addestrato per comprimere tutte le curve del dataset nello spazio latente.
3.  **Clustering**: Applica l'algoritmo K-Means sullo spazio latente per identificare gruppi (cluster) di stili di guida simili.
4.  **Visualizzazione**: Genera grafici 2D/3D dello spazio latente e dei cluster.

> [!IMPORTANT]
> **Nota sul Download Automatico**:
> Il codice include giÃ  dei flag (`download_from_hf = True`) nei file di configurazione (`train.py`, `evaluate.py`) attivati di default. Questo significa che avviando gli script, il dataset **corretto e normalizzato** verrÃ  scaricato automaticamente da Hugging Face, garantendo la totale compatibilitÃ  con i pesi forniti.
>
> **Nota sui Pesi Pre-addestrati**:
> Se si decidesse di non usare il download automatico, per ottenere risultati coerenti utilizzando i pesi inclusi (`src/models/weights/VAE_32z_weights.pth`) e i centroidi (`src/models/weights/kmeans_centroids.npy`), Ã¨ **fondamentale** utilizzare manualmente il dataset specifico disponibile qui:
> [**f1_corner_telemetry_2024_2025/tree/main**](https://huggingface.co/datasets/FlorindoDev/f1_corner_telemetry_2024_2025/tree/main).
>
> L'uso di un dataset diverso o ri-normalizzato localmente renderÃ  i pesi e i centroidi non validi.
>
> **Significato dei Cluster (Configurazione Default)**:
> Utilizzando i pesi e i centroidi inclusi, i 4 cluster identificati hanno il seguente significato:
> - **Cluster 0**: Passo ma veloce.
> - **Cluster 1**: Pushing.
> - **Cluster 2**: Passo ma lento.
> - **Cluster 3**: Saving.

**Cosa puoi fare**:
- **Configurare i percorsi**: Modifica `dataset_path`, `load_weights_path` ecc. in `TrainConfig`.
- **Addestrare da zero o caricare**: Imposta `train_model = True` per riaddestrare, `False` per usare pesi salvati.
- **Scegliere il modello**: `use_vae = True` per Variational AutoEncoder, `False` per AutoEncoder standard.
- **Modificare i cluster**: Cambia `num_clusters` per cercare piÃ¹ o meno stili di guida.
- **Gestire i Centroidi**: Usa `save_centroids = True` per salvare i centroidi calcolati in modo da poterli riutilizzare, oppure `load_centroids = True` per caricare centroidi esistenti senza dover rieseguire il clustering (utile se hai giÃ  dei cluster definiti e vuoi mantenere coerenza).
- **Attivare grafici**: Imposta su `True` flag come `show_latent_space_2d` o `show_clusters_3d` per vedere i risultati.

```bash
python -m src.scripts.train
```

**Download automatico da Hugging Face**:

Puoi abilitare il download automatico del dataset normalizzato impostando `download_from_hf = True` nella config.

### 2. Valutazione Singola Sessione

Lo script `src/scripts/evaluate.py` serve per applicare il modello addestrato a **nuovi dati** (es. un giro specifico di un pilota).

**Come funziona**:
1.  Carica il file di telemetria grezza (`.json`) specificato.
2.  Usa il file `corners.json` per trovare dove sono le curve nel tracciato.
3.  Estrae e normalizza le curve usando le statistiche del dataset originale.
4.  Proietta ogni curva nello spazio latente e le assegna al cluster piÃ¹ vicino.

**Download automatico da Hugging Face**:

Lo script supporta il download automatico dei dati:
- `download_from_hf = True`: Scarica il dataset normalizzato
- `download_raw_from_hf = True`: Scarica i dati telemetrici grezzi (cartella `2024-main` o `2025-main`)

**Cosa puoi fare**:
- **Analizzare un pilota**: Cambia `telemetry_path` per puntare al file JSON della sessione.
- **Confrontare stili**: Esegui lo script su giri diversi per vedere se lo stile cambia.
- **Verificare l'output**: Lo script stampa per ogni curva l'ID del cluster assegnato.

```bash
python -m src.scripts.evaluate
```

### 3. Visualizzazione Curve

Lo script `src/analysis/curve_visualizer.py` permette di **visualizzare le curve** rilevate da un file di telemetria.

**Cosa puoi fare**:
- Vedere graficamente le curve estratte da una sessione
- Visualizzare le traiettorie delle curve sul tracciato
- Scaricare automaticamente i raw data da HF con `download_from_hf = True`

```bash
python -m src.analysis.curve_visualizer
```

### 4. Web Dashboard e Analisi Telemetria

Il progetto include un'interfaccia web completa per analizzare telemetrie, giri e confrontare i piloti.

**Opzione A: Docker (Consigliato)**
Esegui l'intero stack (Backend + Web Server) in un container.

1.  **Avvio**:
    ```bash
    cd site
    docker-compose up --build -d
    ```
2.  **Accesso**: Apri `http://localhost` nel browser.

**Opzione B: Sviluppo Locale**
1.  **Avvio**:
    ```bash
    python site/api_server.py
    ```
2.  **Accesso**: Apri `http://localhost:5050` nel browser.

## Struttura Progetto

```
src/
â”œâ”€â”€ analysis/       # Curve detection, preprocessing e curve_visualizer.py
â”œâ”€â”€ models/         # AutoEncoder/VAE, dataset_loader.py (con download HF)
â””â”€â”€ scripts/        # train.py, evaluate.py
data/
â””â”€â”€ dataset/        # Dataset normalizzati e documentazione
```